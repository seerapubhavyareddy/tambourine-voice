"""Transcription buffer processor for dictation.

Buffers transcription text until the user explicitly stops recording,
then emits a single consolidated transcription for LLM cleanup.

Uses a state machine pattern with tagged unions for explicit state management:
- IdleState: Not recording
- RecordingState: Actively buffering transcriptions
- WaitingForSTTState: Stop received, waiting for STT to catch up
"""

from __future__ import annotations

import asyncio
from dataclasses import dataclass
from datetime import UTC, datetime
from typing import Any

from pipecat.frames.frames import (
    Frame,
    InputTransportMessageFrame,
    OutputTransportMessageFrame,
    TranscriptionFrame,
    UserStartedSpeakingFrame,
    UserStoppedSpeakingFrame,
)
from pipecat.processors.frame_processor import FrameDirection, FrameProcessor
from pipecat.transcriptions.language import Language

from utils.logger import logger

# Maximum time to wait for pending transcription when stop-recording is received
TRANSCRIPTION_WAIT_TIMEOUT_SECONDS = 0.8


# =============================================================================
# State Machine Types
# =============================================================================


@dataclass(frozen=True)
class IdleState:
    """Not recording. Waiting for start-recording message."""

    pass


@dataclass(frozen=True)
class RecordingState:
    """Actively recording and buffering transcriptions."""

    buffer: str = ""
    user_id: str = "user"
    language: Language | None = None
    speech_detected: bool = False


@dataclass(frozen=True)
class WaitingForSTTState:
    """Stop-recording received, waiting for VAD to signal speech has stopped.

    This state is entered when speech was detected when stop-recording arrives.
    We wait for UserStoppedSpeakingFrame from VAD to ensure all pending STT
    transcriptions have been delivered before emitting the final buffer.
    """

    buffer: str
    user_id: str
    language: Language | None
    direction: FrameDirection


# Tagged union of all possible states
State = IdleState | RecordingState | WaitingForSTTState


# =============================================================================
# Processor
# =============================================================================


class TranscriptionBufferProcessor(FrameProcessor):
    """Buffers transcriptions until user stops recording.

    Uses a state machine to manage the recording lifecycle explicitly.
    State transitions are handled via pattern matching, making invalid
    states unrepresentable.
    """

    def __init__(self, **kwargs: Any) -> None:
        """Initialize the transcription buffer processor."""
        super().__init__(**kwargs)
        self._state: State = IdleState()
        self._timeout_task: asyncio.Task[None] | None = None

    async def process_frame(self, frame: Frame, direction: FrameDirection) -> None:
        """Process frames using state machine pattern."""
        await super().process_frame(frame, direction)

        # Handle client messages (start/stop recording)
        if isinstance(frame, InputTransportMessageFrame):
            message_type = self._extract_message_type(frame.message)
            logger.info(f"Received client message: type={message_type}")

            if message_type == "start-recording":
                await self._handle_start_recording()
                return

            if message_type == "stop-recording":
                await self._handle_stop_recording(direction)
                return

        # Handle speech detection
        if isinstance(frame, UserStartedSpeakingFrame):
            self._handle_speech_started()
            await self.push_frame(frame, direction)
            return

        if isinstance(frame, UserStoppedSpeakingFrame):
            await self._handle_speech_stopped(direction)
            await self.push_frame(frame, direction)
            return

        # Handle transcription
        if isinstance(frame, TranscriptionFrame):
            if frame.text:
                await self._handle_transcription(frame, direction)
            return

        # Pass through all other frames unchanged
        await self.push_frame(frame, direction)

    # =========================================================================
    # Message Type Extraction
    # =========================================================================

    def _extract_message_type(self, message: dict[str, Any] | Any) -> str | None:
        """Extract message type from transport message payload."""
        if not isinstance(message, dict):
            return None

        outer_type = message.get("type")
        if outer_type == "client-message":
            data = message.get("data", {})
            if isinstance(data, dict):
                return data.get("t")
        return outer_type

    # =========================================================================
    # State Transition Handlers
    # =========================================================================

    async def _handle_start_recording(self) -> None:
        """Transition to RecordingState from any state."""
        # Cancel any pending timeout from previous WaitingForSTT state
        self._cancel_timeout()
        logger.info("Start-recording received, entering RecordingState")
        self._state = RecordingState()

    async def _handle_stop_recording(self, direction: FrameDirection) -> None:
        """Handle stop-recording based on current state."""
        match self._state:
            case RecordingState(buffer=buffer, speech_detected=speech_detected) as state:
                if speech_detected:
                    # Speech detected - wait for VAD to signal speech stopped
                    # This handles the race condition where STT still has pending
                    # transcriptions when stop-recording arrives
                    logger.info(
                        f"Stop-recording received, waiting for speech to stop "
                        f"(buffer: '{buffer.strip()}')"
                    )
                    self._state = WaitingForSTTState(
                        buffer=state.buffer,
                        user_id=state.user_id,
                        language=state.language,
                        direction=direction,
                    )
                    self._timeout_task = asyncio.create_task(self._stt_timeout_handler(direction))
                else:
                    # No speech detected - send empty response
                    logger.info("Stop-recording received, no speech detected, sending empty")
                    await self._emit_empty_response(direction)
                    self._state = IdleState()

            case WaitingForSTTState():
                # Already waiting - ignore duplicate stop
                logger.warning("Stop-recording received while already waiting for STT")

            case IdleState():
                # Not recording - send empty response
                logger.warning("Stop-recording received while idle")
                await self._emit_empty_response(direction)

    def _handle_speech_started(self) -> None:
        """Mark that speech was detected in current recording."""
        match self._state:
            case RecordingState() as state:
                self._state = RecordingState(
                    buffer=state.buffer,
                    user_id=state.user_id,
                    language=state.language,
                    speech_detected=True,
                )
            case _:
                pass  # Ignore speech events in other states

    async def _handle_speech_stopped(self, direction: FrameDirection) -> None:
        """Handle speech stopped from VAD based on current state."""
        match self._state:
            case WaitingForSTTState(buffer=buffer) as state:
                # Speech stopped while waiting - emit what we have
                self._cancel_timeout()
                if buffer.strip():
                    logger.info(f"Speech stopped, emitting: '{buffer.strip()}'")
                    await self._emit_transcription(state, state.direction)
                else:
                    logger.info("Speech stopped with empty buffer, sending empty")
                    await self._emit_empty_response(state.direction)
                self._state = IdleState()
            case RecordingState():
                # Normal speech stopped during recording - just clear the flag
                # (speech can start/stop multiple times during a recording session)
                pass
            case IdleState():
                pass  # Ignore when idle

    async def _handle_transcription(
        self, frame: TranscriptionFrame, direction: FrameDirection
    ) -> None:
        """Handle incoming transcription based on current state."""
        match self._state:
            case RecordingState() as state:
                # Accumulate transcription
                new_buffer = state.buffer + frame.text
                self._state = RecordingState(
                    buffer=new_buffer,
                    user_id=frame.user_id,
                    language=frame.language,
                    speech_detected=state.speech_detected,
                )
                logger.debug(f"Buffered transcription: '{frame.text}' (total: '{new_buffer}')")

            case WaitingForSTTState() as state:
                # Transcription arrived while waiting - emit immediately
                new_buffer = state.buffer + frame.text
                logger.info(f"Transcription arrived while waiting: '{new_buffer.strip()}'")
                self._cancel_timeout()
                updated_state = WaitingForSTTState(
                    buffer=new_buffer,
                    user_id=frame.user_id,
                    language=frame.language,
                    direction=state.direction,
                )
                await self._emit_transcription(updated_state, state.direction)
                self._state = IdleState()

            case IdleState():
                # Ignore transcriptions when idle (shouldn't happen)
                logger.warning(f"Received transcription while idle: '{frame.text}'")

    # =========================================================================
    # Timeout Handler
    # =========================================================================

    async def _stt_timeout_handler(self, direction: FrameDirection) -> None:
        """Background task that emits buffer after timeout if speech stopped is not received."""
        try:
            await asyncio.sleep(TRANSCRIPTION_WAIT_TIMEOUT_SECONDS)
            # Only act if still in WaitingForSTT state
            match self._state:
                case WaitingForSTTState(buffer=buffer) as state:
                    logger.warning(
                        f"Timeout waiting for speech stopped after "
                        f"{TRANSCRIPTION_WAIT_TIMEOUT_SECONDS}s"
                    )
                    if buffer.strip():
                        logger.info(f"Timeout, emitting buffer: '{buffer.strip()}'")
                        await self._emit_transcription(state, state.direction)
                    else:
                        await self._emit_empty_response(direction)
                    self._state = IdleState()
                case _:
                    pass  # State changed, nothing to do
        except asyncio.CancelledError:
            pass  # Cancelled by speech stopped or new recording

    def _cancel_timeout(self) -> None:
        """Cancel any pending timeout task."""
        if self._timeout_task and not self._timeout_task.done():
            self._timeout_task.cancel()
            self._timeout_task = None

    # =========================================================================
    # Output Helpers
    # =========================================================================

    async def _emit_transcription(
        self, state: RecordingState | WaitingForSTTState, direction: FrameDirection
    ) -> None:
        """Emit the buffered transcription as a consolidated frame."""
        consolidated_frame = TranscriptionFrame(
            text=state.buffer.strip(),
            user_id=state.user_id,
            timestamp=datetime.now(UTC).isoformat(),
            language=state.language,
        )
        await self.push_frame(consolidated_frame, direction)

    async def _emit_empty_response(self, direction: FrameDirection) -> None:
        """Send an empty response message to the client."""
        empty_response = OutputTransportMessageFrame(
            message={
                "label": "rtvi-ai",
                "type": "server-message",
                "data": {"type": "recording-complete", "hasContent": False},
            }
        )
        await self.push_frame(empty_response, direction)
